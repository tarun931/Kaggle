{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":104383,"databundleVersionId":12957508,"sourceType":"competition"},{"sourceId":12559632,"sourceType":"datasetVersion","datasetId":7930680},{"sourceId":12559652,"sourceType":"datasetVersion","datasetId":7930694},{"sourceId":12719174,"sourceType":"datasetVersion","datasetId":8039184},{"sourceId":12729471,"sourceType":"datasetVersion","datasetId":8045877}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile gemma2_inference.py\n\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport os\nfrom IPython.display import display, Math, Latex\nimport torch\nfrom transformers import AutoTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset\nimport pandas as pd, numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import DataLoader\nfrom transformers import DataCollatorWithPadding\nfrom peft import PeftModel\nfrom scipy.special import softmax\nfrom tqdm import tqdm\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nlora_path = \"/kaggle/input/gemma2-9b-it-cv945\"\nMAX_LEN = 256\n# helpers\ndef format_input(row):\n    x = \"Yes\"\n    if not row['is_correct']:\n        x = \"No\"\n    return (\n        f\"Question: {row['QuestionText']}\\n\"\n        f\"Answer: {row['MC_Answer']}\\n\"\n        f\"Correct? {x}\\n\"\n        f\"Student Explanation: {row['StudentExplanation']}\"\n    )\n\n# Tokenization function\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n\nle = LabelEncoder()\n\ntrain = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n\ntrain.Misconception = train.Misconception.fillna('NA')\ntrain['target'] = train.Category+\":\"+train.Misconception\ntrain['label'] = le.fit_transform(train['target'])\ntarget_classes = le.classes_\nn_classes = len(target_classes)\nprint(f\"Train shape: {train.shape} with {n_classes} target classes\")\nidx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'\ncorrect = train.loc[idx].copy()\ncorrect['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\ncorrect = correct.sort_values('c',ascending=False)\ncorrect = correct.drop_duplicates(['QuestionId'])\ncorrect = correct[['QuestionId','MC_Answer']]\ncorrect['is_correct'] = 1\n\n# Prepare test data\ntest = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\ntest = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\ntest.is_correct = test.is_correct.fillna(0)\ntest['text'] = test.apply(format_input, axis=1)\n\n\n# load model & tokenizer\ntokenizer = AutoTokenizer.from_pretrained(lora_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"/kaggle/input/gemma2-9b-it-bf16\",\n    num_labels=n_classes,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nmodel = PeftModel.from_pretrained(model, lora_path)\nmodel.eval()\n\nds_test = Dataset.from_pandas(test[['text']])\nds_test = ds_test.map(tokenize, batched=True, remove_columns=['text'])\n\ndata_collator = DataCollatorWithPadding(\n    tokenizer=tokenizer,\n    max_length=MAX_LEN,  \n    return_tensors=\"pt\")\n\ndataloader = DataLoader(\n    ds_test,\n    batch_size=8,  \n    shuffle=False,\n    collate_fn=data_collator,\n    pin_memory=True,  \n    num_workers=2     \n)\n\nall_logits = []\ndevice = next(model.parameters()).device\n\nwith torch.no_grad():\n    for batch in tqdm(dataloader, desc=\"Inference\"):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        logits = outputs.logits\n        all_logits.append(logits.float().cpu().numpy())\n\npredictions = np.concatenate(all_logits, axis=0)\n\nprobs = softmax(predictions, axis=1)\n\ntop_indices = np.argsort(-probs, axis=1)\nflat_indices = top_indices.flatten()\ndecoded_labels = le.inverse_transform(flat_indices)\ntop_labels = decoded_labels.reshape(top_indices.shape)\njoined_preds = [\" \".join(row[:3]) for row in top_labels]\n\nsub = pd.DataFrame({\n    \"row_id\": test.row_id.values,\n    \"Category:Misconception\": joined_preds\n})\nsub.to_csv(\"submission_gemma.csv\", index=False)\n\nprob_data = []\nfor i in range(len(test)):\n    prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(25)}  # Top 25\n    prob_dict['row_id'] = test.row_id.values[i]\n    prob_dict['top_classes'] = \" \".join(top_labels[i, :25])  # Top 25 class names\n    prob_data.append(prob_dict)\n\nprob_df = pd.DataFrame(prob_data)\nprob_df.to_csv(\"submission_gemma_prob.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T14:19:20.331353Z","iopub.execute_input":"2025-09-15T14:19:20.33161Z","iopub.status.idle":"2025-09-15T14:19:20.342307Z","shell.execute_reply.started":"2025-09-15T14:19:20.331588Z","shell.execute_reply":"2025-09-15T14:19:20.341456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile qwen3_deepseek_inference.py\n\nimport os\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom datasets import Dataset\nimport threading\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\nfrom scipy.special import softmax\nfrom tqdm import tqdm\nimport time\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\ntrain = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\ntest  = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n\nmodel_paths = [\n    \"/kaggle/input/deekseepmath-7b-map-competition/MAP_EXP_09_FULL\",\n   \"/kaggle/input/qwen3-8b-map-competition/MAP_EXP_16_FULL\"]\n\ndef format_input(row):\n    x = \"This answer is correct.\"\n    if not row['is_correct']:\n        x = \"This is answer is incorrect.\"\n    return (\n        f\"Question: {row['QuestionText']}\\n\"\n        f\"Answer: {row['MC_Answer']}\\n\"\n        f\"{x}\\n\"\n        f\"Student Explanation: {row['StudentExplanation']}\")\n\n\nle = LabelEncoder()\ntrain.Misconception  = train.Misconception.fillna('NA')\ntrain['target']   = train.Category + ':' +train.Misconception\ntrain['label']    = le.fit_transform(train['target'])\n\nn_classes = len(le.classes_)\nprint(f\"Train shape: {train.shape} with {n_classes} target classes\")\nidx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'\ncorrect = train.loc[idx].copy()\ncorrect['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\ncorrect = correct.sort_values('c',ascending=False)\ncorrect = correct.drop_duplicates(['QuestionId'])\ncorrect = correct[['QuestionId','MC_Answer']]\ncorrect['is_correct'] = 1\n\ntest = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\ntest.is_correct = test.is_correct.fillna(0)\ntest['text'] = test.apply(format_input,axis=1)\nds_test = Dataset.from_pandas(test)\n\n\ndef run_inference_on_gpu(model_path, gpu_id, test_data, output_name):\n    \"\"\"Run inference for one model on one GPU\"\"\"\n    \n    device = f\"cuda:{gpu_id}\"\n    print(f\"Loading {output_name} on {device}...\")\n    \n    # Load model\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_path, \n        device_map=device, \n        torch_dtype=torch.float16\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model.config.pad_token_id = tokenizer.pad_token_id\n    model.eval()\n    \n    # Tokenize function\n    def tokenize(batch):\n        return tokenizer(batch[\"text\"], \n                        truncation=True,\n                        max_length=256)\n    \n    ds_test = Dataset.from_pandas(test_data[['text']])\n    ds_test = ds_test.map(tokenize, batched=True, remove_columns=['text'])\n    data_collator = DataCollatorWithPadding(\n        tokenizer=tokenizer,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n\n    dataloader = DataLoader(\n        ds_test,\n        batch_size=4,\n        shuffle=False,\n        collate_fn=data_collator,\n        pin_memory=True,\n        num_workers=0\n    )\n\n    all_logits = []\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=f\"{output_name}\"):\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            all_logits.append(outputs.logits.float().cpu().numpy())\n    \n    predictions = np.concatenate(all_logits, axis=0)\n\n    probs = softmax(predictions, axis=1)\n    top_indices = np.argsort(-probs, axis=1)\n\n    flat_indices = top_indices.flatten()\n    decoded_labels = le.inverse_transform(flat_indices)\n    top_labels = decoded_labels.reshape(top_indices.shape)\n    joined_preds = [\" \".join(row[:3]) for row in top_labels]\n    sub = pd.DataFrame({\n        \"row_id\": test_data.row_id.values,\n        \"Category:Misconception\": joined_preds\n    })\n    sub.to_csv(f\"submission_{output_name}.csv\", index=False)\n\n    prob_data = []\n    for i in range(len(predictions)):\n        prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(25)}\n        prob_dict['row_id'] = test_data.row_id.values[i]\n        prob_dict['top_classes'] = \" \".join(top_labels[i, :25])\n        prob_data.append(prob_dict)\n    \n    prob_df = pd.DataFrame(prob_data)\n    prob_df.to_csv(f\"submission_{output_name}_probabilities.csv\", index=False)\n    \n    print(f\" {output_name} completed - saved submission and probabilities\")\n    del model, tokenizer\n    torch.cuda.empty_cache()\n\nprint(\" Starting multi-GPU inference...\")\nstart_time = time.time()\n\nthreads = []\ngpu_assignments = [\n    (model_paths[0], 0, \"deepseek\"),\n    (model_paths[1], 1, \"qwen3\"),\n]\n\nfor model_path, gpu_id, name in gpu_assignments:\n    if gpu_id < torch.cuda.device_count():  \n        thread = threading.Thread(\n            target=run_inference_on_gpu,\n            args=(model_path, gpu_id, test, name)\n        )\n        threads.append(thread)\n        thread.start()\n        time.sleep(10) \n\nfor thread in threads:\n    thread.join()\n\nend_time = time.time()\nprint(f\" completed in {end_time - start_time:.2f} seconds!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T14:19:37.441445Z","iopub.execute_input":"2025-09-15T14:19:37.441701Z","iopub.status.idle":"2025-09-15T14:19:37.44849Z","shell.execute_reply.started":"2025-09-15T14:19:37.441681Z","shell.execute_reply":"2025-09-15T14:19:37.447948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time \n!python /kaggle/working/gemma2_inference.py\ntime.sleep(20)\n!python /kaggle/working/qwen3_deepseek_inference.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T14:19:45.174922Z","iopub.execute_input":"2025-09-15T14:19:45.175174Z","iopub.status.idle":"2025-09-15T14:24:46.270884Z","shell.execute_reply.started":"2025-09-15T14:19:45.175155Z","shell.execute_reply":"2025-09-15T14:24:46.269952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom scipy.special import softmax\ndef extract_class_probabilities(row, model_suffix='', top_k=25):\n    \"\"\"Extract class names and probabilities from a row\"\"\"\n    # Get top classes\n    classes_col = f'top_classes{model_suffix}'\n    if classes_col in row:\n        classes = row[classes_col].split(' ')[:top_k]\n    else:\n        return {}\n    # Get probabilities\n    class_probs = {}\n    for i in range(min(top_k, len(classes))):\n        prob_col = f'prob_{i}{model_suffix}'\n        if prob_col in row:\n            class_probs[classes[i]] = row[prob_col]\n    return class_probs\n\n\ndef ensemble_with_disagreement_handling(prob_files, model_weights=None, top_k=3):\n    n_models = len(prob_files)\n    prob_dfs = []\n    final_predictions = []\n    \n    for file_path in prob_files:\n        df = pd.read_csv(file_path)\n        prob_dfs.append(df)\n    \n    merged_df = prob_dfs[0]\n    for i, df in enumerate(prob_dfs[1:], 1):\n        merged_df = pd.merge(merged_df, df, on='row_id', suffixes=('', f'_model{i+1}'))\n      \n    for idx, row in merged_df.iterrows():\n\n        all_class_probs = []\n        for i in range(n_models):\n            suffix = f'_model{i+1}' if i > 0 else ''\n            class_probs = extract_class_probabilities(row, suffix, top_k=25)\n            all_class_probs.append(class_probs)\n        all_classes = set()\n        for class_probs in all_class_probs:\n            all_classes.update(class_probs.keys())\n\n        class_votes = defaultdict(int)\n        class_total_prob = defaultdict(float)\n        class_max_prob = defaultdict(float)\n        \n        for i, class_probs in enumerate(all_class_probs):\n            weight = model_weights[i]\n            \n            for class_name, prob in class_probs.items():\n                class_votes[class_name] += 1\n                class_total_prob[class_name] += prob * weight\n                class_max_prob[class_name] = max(class_max_prob[class_name], prob * weight)\n        \n        final_scores = {}\n        for class_name in all_classes:\n            base_score = class_total_prob[class_name]\n            agreement_bonus = class_votes[class_name] / n_models\n\n            confidence_bonus = class_max_prob[class_name]\n\n            final_scores[class_name] = (\n                base_score * 0.6 +           \n                agreement_bonus * 0.3 +      \n                confidence_bonus * 0.1       \n            )\n        \n        sorted_classes = sorted(final_scores.items(), key=lambda x: -x[1])\n        top_classes = [class_name for class_name, _ in sorted_classes[:top_k]]\n        \n        final_predictions.append(' '.join(top_classes))\n    \n    return final_predictions\n\n\nw1 = 1.2\nw2 = 1.0\nw3 = 0.8\n\nprob_files = [\n    '/kaggle/working/submission_deepseek_probabilities.csv',\n    '/kaggle/working/submission_gemma_prob.csv',\n        '/kaggle/working/submission_qwen3_probabilities.csv'\n\n]\n\npredictions = ensemble_with_disagreement_handling(\n        prob_files, \n        model_weights=[w1, w2, w3],  \n        top_k=3\n    )\n    \ntest_df = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n\nsubmission = pd.DataFrame({\n    'row_id': test_df.row_id.values,\n    'Category:Misconception': predictions\n})\n\nsubmission.to_csv('/kaggle/working/submission.csv', index=False)\nsubmission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T14:25:04.465172Z","iopub.execute_input":"2025-09-15T14:25:04.465467Z","iopub.status.idle":"2025-09-15T14:25:05.022058Z","shell.execute_reply.started":"2025-09-15T14:25:04.465438Z","shell.execute_reply":"2025-09-15T14:25:05.021467Z"}},"outputs":[],"execution_count":null}]}